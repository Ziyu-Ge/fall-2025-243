{
  "hash": "4e1e177fd226bf5f84bc8d7d8061fc79",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Problem Set 4\"\nsubtitle: \"Due Monday Oct. 14, 10 am\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: false\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: false\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n\n\n\n\n\n\n## Comments\n\n- This covers material in Unit 5, Sections 7-9.\n- It's due at 10 am (Pacific) on Monday October 14, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\n- Please see PS1 for formatting and attribution requirements.\n\n## Problems\n\n\n1. *Memoization* of a function involves storing (caching) the values produced by a given input and then returning the cached result instead of rerunning the function when the same input is provided in the future. It can be a good approach to improve efficiency when a calculation is expensive (and presuming that sometimes the function will be run with inputs on whic it has already been used). It's particularly useful for for recursive calculuations that involve solving a subproblem in order to solve a given (larger) problem. If you've already solved the subproblem you don't need to solve it again. For example if one is working with graphs or networks, one often ends up writing recursive algorithms. (Suppose, e.g., you have a geneology giving relationships of parents and children. If you wanted to find all the descendants of a person, and you had already found all the descendants of one of the person's children, you could make use of that without finding the descendants of the child again.)\n\n   Write a decorator that implements memoization. It should handle functions with either one or two arguments (write one decorator, not two!). You can assume these arguments are simple objects like numbers or strings. As part of your solution, explain whether you need to use `nonlocal` or not in this case. \n\n   Test your code on basic cases, such as applying the log-gamma function to a number and multiplying together two numbers. These may not be realistic cases, because the lookup process could well take more time than the actual calculation. To assess that, time the memoization approach compared to directly doing the calculation. Be careful that you are getting an accurate timing of such quick calculations (see Unit 5 notes)! \n\n2. Let's work more on how Python stores and copies objects, in particular lists.\n\n   a. Experiment with creating lists of real-valued numbers of different lengths. Try to determine how much memory is used for each reference (each element of the list), not counting the memory used for the actual values. Also consider lists of more complicated objects to see if the behavior is the same or similar. \n\n   b. Use the list `.copy` method with a list of numbers and with a list containing more complicated objects (including another list or some dictionary). Compare the behavior in terms of what happens when you modify elements at different levels of nestedness. Relate the behavior to the help information in `help(list.copy)` and to what we know about how the structure of lists.\n\n   c. Consider creating these lists.\n\n\n      ::: {.cell execution_count=1}\n      ``` {.python .cell-code}\n      my_int = 1\n      my_real = 1.0\n      my_string = 'hat'\n      \n      xi = [1, my_int, my_int, 2]\n      yi = [1, my_int, my_int, 2]\n      xr = [1.0, my_real, my_real, 2.0]\n      yr = [1.0, my_real, my_real, 2.0]\n      \n      xc = ['hat', my_string, my_string, 'dog']\n      yc = ['hat', my_string, my_string, 'dog']\n      ```\n      :::\n      \n      \n      What is different about how the elements in these lists are stored?\n\n3. Suppose I want to compute the trace of a matrix, $A$, where $A=XY$. The trace is $\\sum_{i=1}^{n} A_{ii}$. Assume both $X$ and $Y$ are $n \\times n$. A naive implementation is `np.sum(np.diag(X@Y))`.\n\n   a. What is the computational complexity of that naive implementation: $O(n)$, $O(n^2)$, or $O(n^3)$? You can just count up the number of multiplications and ignore the additions. Why is that naive implementation inefficient?\n   b. Write Python code that (much) more eï¬€iciently computes the trace using vectorized/matrix operations on the matrices. You will not be able to use `map` or list comprehension to achieve this speedup. What is the computational complexity of your solution?\n   c. Create a plot, as a function of $n$, to demonstrate the scaling of the original implementation compared to your improved implementation.\n   d. (Extra credit) Implement your more efficient version in using Jax (see Section 9 of Unit 5) and compare the timing to the numpy implementation, both with and without using `jax.jit()`.\n\n4. Suppose we have a matrix in which each column is a vector\nof probabilities that add to one, and we want to generate a sample from\nthe categorical distribution represented by the probabilities in each column.\nFor example, the first column might be (0.9, 0.05, 0.05)\nand the second column might be (0.1, 0.85, .0.5). When we generate the\nfirst sample, it is very likely to be a 1 (a 0 in Python) and the second sample is very\nlikely to be a 2 (a 1 in Python). We could do this using a for loop over the columns of the\nmatrix, and `np.random.choice()`, but that is a lot slower than some\nother ways we might do it because it is a loop executing in Python over many\nelements (columns).\n\n\n   ::: {.cell execution_count=2}\n   ``` {.python .cell-code}\n   import numpy as np\n   import time\n   \n   n = 100000\n   p = 5\n   \n   # Generate a random matrix and calculate probabilities.\n   np.random.seed(1)\n   tmp = np.exp(np.random.randn(p, n))\n   probs = tmp / tmp.sum(axis=0)\n   \n   smp = np.zeros(n, dtype=int)\n   \n   # Generate sample by column.\n   np.random.seed(1)\n   start = time.time()\n   for i in range(n):\n       smp[i] = np.random.choice(p, size=1, p=probs[:,i])[0]\n   print(f\"Loop by column time: {round(time.time() - start, 2)} seconds.\")\n   ```\n   \n   ::: {.cell-output .cell-output-stdout}\n   ```\n   Loop by column time: 2.88 seconds.\n   ```\n   :::\n   :::\n   \n   \n   a. Consider transposing the matrix and looping over rows. Why might I hypothesize that this could be faster? Is it faster? Why does it make sense that you got the timing result that you got? Does using numpy's `apply` functionality help at all?\n\n   b. How can we do it **much** faster (multiple orders of magnitude), exploiting vectorization? (Hint: This might involve some looping or not, but not in the ways described above. Think about how one can use random uniform numbers to generate from a categorical distribution.)\n\n",
    "supporting": [
      "ps4_files/figure-pdf"
    ],
    "filters": []
  }
}